# Slurm Workload Manager

## Introduction
Slurm (Simple Linux Utility for Resource Management) is a widely used open-source workload manager designed for job scheduling and resource management in high-performance computing (HPC) clusters. It is utilized by some of the largest supercomputers worldwide to efficiently allocate computing resources, manage job queues, and optimize cluster performance.

## Key Features
- **Job Scheduling**: Allocates resources to submitted jobs based on priority, fairness, and resource availability.
- **Resource Management**: Assigns CPUs, memory, and GPUs efficiently to maximize utilization.
- **Scalability**: Supports small clusters to exascale supercomputers.
- **Fault Tolerance**: Provides job checkpointing and failure recovery mechanisms.
- **Flexible Job Control**: Users can submit, monitor, and cancel jobs as needed.

## Slurm Terminology
- **Job**: A single workload submitted for execution.
- **Task**: A unit of execution within a job.
- **Node**: A physical server within an HPC cluster.
- **Partition**: A logical grouping of nodes, similar to a queue.
- **Core**: A processing unit within a CPU that handles computations.
- **Thread**: The smallest sequence of instructions processed by a core.
- **Reservation**: A block of pre-allocated resources for specific jobs or users.

## Basic Slurm Workflow
1. **User Actions**:
   - Log into the HPC cluster.
   - Create a job script defining resource requirements.
   - Submit the job to Slurm.
   - Monitor job progress.

2. **Slurm Actions**:
   - Accept job submissions.
   - Allocate available resources based on job priority.
   - Execute jobs on assigned nodes.
   - Track resource usage and manage job states.

## Slurm Job States
- **PENDING (PD)**: Job is in the queue waiting for resources.
- **RUNNING (R)**: Job is actively executing.
- **COMPLETED (CD)**: Job finished successfully.
- **FAILED (F)**: Job terminated due to an error.
- **TIMEOUT (TO)**: Job exceeded the allocated time limit.
- **CANCELLED (CA)**: Job was manually stopped by the user.

## Common Slurm Commands
| Command | Description |
|---------|-------------|
| `sinfo` | Displays cluster node and partition status. |
| `sbatch script.sh` | Submits a batch job script. |
| `srun` | Runs a command interactively on allocated resources. |
| `squeue` | Lists jobs currently in the queue. |
| `scancel job_id` | Cancels a running or pending job. |
| `sacct` | Displays accounting information for past jobs. |

## Example Slurm Job Script
```bash
#!/bin/bash
#SBATCH --job-name=test_job      # Job name
#SBATCH --partition=128GB        # Partition (queue) name
#SBATCH --nodes=1                # Number of nodes
#SBATCH --ntasks=1               # Number of tasks
#SBATCH --time=01:00:00          # Time limit (HH:MM:SS)
#SBATCH --output=job_output.log  # Output file
#SBATCH --error=job_error.log    # Error file

module load python/3.8  # Load required modules
python my_script.py     # Execute script
```

## Troubleshooting Slurm Jobs
- **Check job status:** `squeue -u your_username`
- **View job logs:** Check the `.out` and `.err` files.
- **Inspect job details:** `scontrol show job job_id`
- **Debug interactively:** Use `srun --pty /bin/bash` to start an interactive session.
- **Optimize resource requests:** Ensure memory, CPU, and time requests match job needs.

## Additional Resources
- [Official Slurm Documentation](https://slurm.schedmd.com/)
- [HPC Cluster User Guide](https://your-cluster-docs.example.com)
- [Slurm Commands Cheat Sheet](https://cheat-sheet-link.example.com)

## License
This repository is provided under the MIT License. It is an open-source guide intended for educational and informational purposes.


