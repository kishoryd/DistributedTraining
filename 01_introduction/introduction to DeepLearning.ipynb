{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLa1EZq42zf0"
   },
   "source": [
    "# Image Classification with the MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-nDnmoC2zf1"
   },
   "source": [
    "In this section we will do the \"Hello World\" of deep learning: training a deep learning model to correctly classify hand-written digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWd9Mjxm2zf1"
   },
   "source": [
    "Objectives of this excercise :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmkmljVQ2zf1"
   },
   "source": [
    "---\n",
    "* Explore how deep learning addresses challenges that are difficult to solve with traditional programming techniques.\n",
    "* Get acquainted with the MNIST dataset of handwritten digits.\n",
    "* Utilize torchvision to load and preprocess the MNIST dataset for model training.\n",
    "* Design a straightforward neural network for image classification tasks.\n",
    "* Train the neural network using the prepared MNIST dataset.\n",
    "* Evaluate how the trained neural network performs on the classification task.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a clear and beginner-friendly explanation to introduce loading the libraries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "siboVxRJ5pes"
   },
   "outputs": [],
   "source": [
    "# Core PyTorch library for tensor operations and GPU acceleration\n",
    "import torch\n",
    "\n",
    "# PyTorch module for building neural networks\n",
    "import torch.nn as nn\n",
    "\n",
    "# Dataset and DataLoader for managing and batching datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Adam optimizer for updating model weights\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Torchvision library for prebuilt datasets and vision utilities\n",
    "import torchvision\n",
    "torchvision.disable_beta_transforms_warning()\n",
    "\n",
    "# Transforms for preprocessing and augmenting image datasets\n",
    "import torchvision.transforms.v2 as transforms\n",
    "\n",
    "# Functional API for low-level image transformations\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "# Matplotlib for visualizing images and plotting results\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQJHCNcy5_Mk"
   },
   "source": [
    "In PyTorch, we can use our GPU in our operations by setting the [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device) to `cuda`. The function `torch.cuda.is_available()` will confirm PyTorch can recognize the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1714980671199,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "AgBjq0uu6CYm",
    "outputId": "9a881df2-6d26-4c95-d44b-1730d4d5b150"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Check and set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ygLT32E-2zf1"
   },
   "source": [
    "---\n",
    "\n",
    "In traditional programming, developers explicitly define rules and conditions for a program to follow, allowing it to perform tasks accurately. This method works effectively for solving a wide range of problems.\n",
    "\n",
    "However, tasks like image classification—which involves assigning an unseen image to its appropriate category—pose significant challenges for traditional programming. It is impractical for a programmer to manually define rules that can account for the vast diversity of images and scenarios, especially for cases they have never encountered before.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ch148qeW2zf2"
   },
   "source": [
    "### Solution to problem is Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdR-fe3Y2zf2"
   },
   "source": [
    "Deep learning excels at pattern recognition by trial and error. By training a deep neural network with sufficient data, and providing the network with feedback on its performance via training, the network can identify, though a huge amount of iteration, its own set of conditions by which it can act in the correct way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8e6Tasm2zf2"
   },
   "source": [
    "## Data: The MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUopNy9V2zf2"
   },
   "source": [
    "---\n",
    "\n",
    "In the evolution of deep learning, achieving accurate image classification on the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) marked a significant milestone. This dataset, consisting of 70,000 grayscale images of handwritten digits (0 through 9), played a pivotal role in demonstrating the potential of neural networks. Although this task is now considered straightforward, working with MNIST has become the deep learning equivalent of a \"Hello World\" program, serving as an introductory benchmark for beginners.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ksf2B052zf2"
   },
   "source": [
    "### Training and Validation Data and Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8d-FSGUU2zf2"
   },
   "source": [
    "---\n",
    "\n",
    "When working with images in deep learning, we require both the image data, typically represented as `X`, and their corresponding [labels](https://developers.google.com/machine-learning/glossary#label), represented as `Y`. These pairs of `X` and `Y` values are essential for both *training* the model and *validating* its performance after training.\n",
    "\n",
    "A helpful analogy is to think of `X` and `Y` pairs as flashcards. A student uses one set of flashcards to study and learns from them. To check their understanding, a teacher might test them using a different set of flashcards. Similarly, in deep learning, we divide the data into distinct sets for training and validation.\n",
    "\n",
    "For the MNIST dataset, the data is divided into four segments:\n",
    "1. **`x_train`**: Images used for training the neural network.\n",
    "2. **`y_train`**: Correct labels corresponding to the `x_train` images, used to measure the model’s accuracy during training.\n",
    "3. **`x_valid`**: A separate set of images reserved for validating the model’s performance after training.\n",
    "4. **`y_valid`**: Correct labels for the `x_valid` images, used to evaluate the model’s predictions on the validation set.\n",
    "\n",
    "The process of organizing and preparing data for analysis is often referred to as [Data Engineering](https://medium.com/@rchang/a-beginners-guide-to-data-engineering-part-i-4227c5c457d7). For a more detailed explanation of the differences between training, validation, and test data, you can refer to [this resource](https://machinelearningmastery.com/difference-test-validation-datasets/) by Jason Brownlee.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exXV58Gn2zf2"
   },
   "source": [
    "### Loading the Data Into Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rp-63AGt2zf2"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "There are various [deep learning frameworks](https://developer.nvidia.com/deep-learning-frameworks) available, each offering unique features and benefits. In this tutorial, we will be using [PyTorch 2](https://pytorch.org/get-started/pytorch-2.0/), with a focus on the [Sequential API](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html). The Sequential API simplifies the process of building neural networks by providing a collection of built-in functions. It is also a popular choice in professional deep learning workflows due to its [clarity](https://blog.pragmaticengineer.com/readable-code/) and performance. While PyTorch is widely used, exploring other frameworks can be valuable when starting a deep learning project.\n",
    "\n",
    "Additionally, we will leverage the [TorchVision](https://pytorch.org/vision/stable/index.html) library, which offers tools to streamline deep learning projects. Among its features are modules that provide convenient access to [common datasets](https://pytorch.org/vision/main/datasets.html), such as MNIST.\n",
    "\n",
    "To begin, we will load the `train` and `valid` datasets for [MNIST](https://pytorch.org/vision/main/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4382,
     "status": "ok",
     "timestamp": 1714980675579,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "If_AfbCG2zf3",
    "outputId": "8d7266b2-7b9f-4f9d-a0cd-ae38ed0840f9"
   },
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.MNIST(\"./data\", train=True, download=False)\n",
    "valid_set = torchvision.datasets.MNIST(\"./data\", train=False, download=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We stated above that the MNIST dataset contained 70,000 grayscale images of handwritten digits. By executing the following cells, we can see that Keras has partitioned 60,000 of these [PIL Images](https://pillow.readthedocs.io/en/stable/reference/Image.html) for training, and 10,000 for validation (after training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 156,
     "status": "ok",
     "timestamp": 1714980675725,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "tv9vHMDZNNs3",
    "outputId": "6fe55662-e37a-427c-e055-08e5dcdff18b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./data\n",
       "    Split: Train"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1714980675726,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "vEKzOpJpNPIS",
    "outputId": "39715875-6a29-4429-fdfd-cbd95221fdd9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: ./data\n",
       "    Split: Test"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the first x, y pair from `train_set` and review the data structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_10, y_10 = train_set[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+vRPAHwnufHukXOoW+t2dqIZPK8lkZ33cEbhxgHPBGa5XxX4Zv/CHiK50bUAvmwnKuv3ZEP3WHsRWLRXt3wEt9PtJrrVp/FUFkwcRzaZJsQSqASjMz9eSSNvTHXmue+PcPl/FK5fy0UTW0LhlYkv8u3J9D8uOOwB715lWhodtYXmuWVtql09rYyzKs86AEop788fienWvVNE+ByXmojUZfEmlz+G4Ss8k8Up3tD97DDACHaOSTxk+lcb8TvFlv4v8ZTXlguzTbeNbazGzb+7XvjsCSSB2BArjaKKKK//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA6UlEQVR4AWNgoD9gRLJSyyf19AWGCb+QhODM9E//QMAJLoDMEHoBlnznhiwIZ2d8+fcAKN8LF0BhnP93CSiphCIG54ScAxmsCeejMiQuAiVXw8VY4CwgI1pPB0geRRaCsTWu/QKZimQnE0yKgUFTEWJMAUIIiZX3DawTu52TbgswsEzmQ1KOxmRs+HdbHk0MzmX/9++aDJyHxuj6968ETYhBeGMUWEjyA7JXoKqW/Ltur8JgHHnu379uDnSdlkf//bu35eO/f3+vcqPLMTD0ZIL9+O8NshQsbEvYeRgMIhk+Yo9pZB00ZgMABFdhZcdK6pUAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34dQ8RKFNf4z"
   },
   "source": [
    "*Note*: The `Split` for `valid_set` is stated as `Test`, but we will be using the data for validation in our hands-on exercises. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "826TKJQaOOVs"
   },
   "source": [
    "### Prepare image for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZ_zoaub2zf4"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "In data science, a vector is considered a 1-dimensional array, while a matrix is a 2-dimensional array. Extending this concept, a tensor is an n-dimensional array that can represent data with any number of dimensions. Modern neural network frameworks like PyTorch are designed to process tensors efficiently.\n",
    "\n",
    "A good example of a 3-dimensional tensor is an image, where the dimensions can correspond to width, height, and color channels (such as RGB). This idea of tensor operations is similar to how video games calculate pixel values using matrix mathematics, which is why GPUs (Graphics Processing Units) are highly effective at processing tensors.\n",
    "\n",
    "To prepare images for neural network training, they need to be converted into tensors. TorchVision offers a simple method for this conversion using the `ToTensor` class. This class can convert [PIL Images](https://pillow.readthedocs.io/en/stable/reference/Image.html) or NumPy arrays into PyTorch tensors, automatically scaling pixel values to the range [0, 1].\n",
    "\n",
    "--- \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "uWnZxkiqUDp6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samritm/.conda/envs/gujcost_workshop/lib/python3.9/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `transforms.Compose([transforms.ToImageTensor(), transforms.ConvertImageDtype()])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()  # Convert image to tensor and normalize to [0, 1]\n",
    "])\n",
    "\n",
    "# Apply the transformation\n",
    "x_10_tensor = transform(x_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 152,
     "status": "ok",
     "timestamp": 1714980703279,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "F-jyN9_N2zf4",
    "outputId": "ea7a13bb-0f4b-4792-df28-7a21812acecd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(x_10_tensor.dtype) #verify the datatype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIDUGa4LYzYb"
   },
   "source": [
    "We can verify the minimum and maximum values. PIL Images have a potential integer range of [0, 255], but the [ToTensor](https://pytorch.org/vision/main/generated/torchvision.transforms.ToTensor.html) class converts it to a float range of [0.0, 1.0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 156,
     "status": "ok",
     "timestamp": 1714980704869,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "XTTjlZVS2zf4",
    "outputId": "28be071a-3d2c-496e-ae07-e788ae6a284b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(x_10_tensor.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 210,
     "status": "ok",
     "timestamp": 1714980706159,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "H_denh2i2zf4",
    "outputId": "98b3f276-cc73-46dc-cb21-0005d2756a28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9961)\n"
     ]
    }
   ],
   "source": [
    "print(x_10_tensor.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9A9EL-34ZpSS"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "In PyTorch, the dimensions of an image tensor follow the `C x H x W` format:\n",
    "- `C`: Number of color channels.\n",
    "- `H`: Height of the image.\n",
    "- `W`: Width of the image.\n",
    "\n",
    "For grayscale images like MNIST, there is a single color channel (`C=1`). Each image is square-shaped, with a height (`H`) and width (`W`) of 28 pixels.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1714980707121,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "cIxHEBDRZk-K",
    "outputId": "e6fbc8d8-1bdb-42c1-8a30-6c022daa7d65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_10_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1714980708660,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "CLU8kY332zf4",
    "outputId": "7e8fbef2-d1a9-4910-a1a9-caad1cec750c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.1647, 0.4627, 0.8588, 0.6510, 0.4627,\n",
       "          0.4627, 0.0235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.4039, 0.9490, 0.9961, 0.9961, 0.9961, 0.9961,\n",
       "          0.9961, 0.2588, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0706, 0.9098, 0.9961, 0.9961, 0.9961, 0.9961,\n",
       "          0.9961, 0.9333, 0.2745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.4078, 0.9569, 0.9961, 0.8784, 0.9961,\n",
       "          0.9961, 0.9961, 0.5529, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.8118, 0.9961, 0.8235, 0.9961,\n",
       "          0.9961, 0.9961, 0.1333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.3294, 0.8078, 0.9961, 0.9961,\n",
       "          0.9961, 0.9961, 0.1608, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0941, 0.8196, 0.9961,\n",
       "          0.9961, 0.9961, 0.6706, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.3569, 0.5373, 0.9922, 0.9961,\n",
       "          0.9961, 0.9961, 0.4392, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.1569, 0.8392, 0.9804, 0.9961, 0.9961, 0.9961,\n",
       "          0.9961, 0.9961, 0.1333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.3176, 0.9686, 0.9961, 0.9961, 0.9961, 0.9961,\n",
       "          0.9961, 0.9961, 0.5725, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.4314, 0.9647, 0.9961, 0.9961, 0.9961,\n",
       "          0.9961, 0.9961, 0.6706, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.2863, 0.3490, 0.3490, 0.3647,\n",
       "          0.9412, 0.9961, 0.6706, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039,\n",
       "          0.5020, 0.9961, 0.8588, 0.1216, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0275,\n",
       "          0.9961, 0.9961, 0.8392, 0.1098, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5412,\n",
       "          0.9961, 0.9961, 0.4549, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0745, 0.6941,\n",
       "          0.3529, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0980, 0.9412,\n",
       "          0.9961, 0.9961, 0.1333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6431, 0.9961,\n",
       "          0.8431, 0.2471, 0.1412, 0.0000, 0.2000, 0.3490, 0.8078, 0.9961,\n",
       "          0.9961, 0.5451, 0.0314, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2235, 0.7725,\n",
       "          0.9961, 0.9961, 0.8706, 0.7059, 0.9451, 0.9961, 0.9961, 0.9922,\n",
       "          0.8353, 0.0431, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5490,\n",
       "          0.4118, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9255,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0275, 0.4588, 0.4588, 0.6471, 0.9961, 0.9961, 0.9373, 0.1961,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "          0.0000, 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_10_tensor #values stored inside the variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, a tensor is processed with a [CPU](https://www.arm.com/glossary/cpu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 145,
     "status": "ok",
     "timestamp": 1714980711055,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "MUDWAzz386wC",
    "outputId": "e4610927-74da-4a7d-cac8-3836661aaf2c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_10_tensor.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To move it to a GPU, we can use the `.cuda` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 208,
     "status": "ok",
     "timestamp": 1714980712295,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "PtVbrIK_8bwz",
    "outputId": "7e9256b6-8cd1-4a14-e656-114298564a04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_10_gpu = x_10_tensor.cuda() # will throw error if no cuda driver or cudatoolkit is found underneath\n",
    "x_10_gpu.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The `.cuda` method can only be used if PyTorch detects a compatible GPU. To make our code more adaptable, we can use the `to` method with the `device` variable defined earlier. This approach ensures that the tensor is sent to the appropriate device—GPU if available, or CPU otherwise. By doing this, our code takes advantage of GPU acceleration when possible, while still functioning correctly on systems without a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 144,
     "status": "ok",
     "timestamp": 1714980713216,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "_qSz_fcP8swy",
    "outputId": "db53c12e-8809-4023-fd9b-bdd5c1b39b4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_10_tensor.to(device).device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBqQfpwo2zf4"
   },
   "source": [
    "Sometimes, it can be hard to interpret so many numbers. Thankfully, TorchVision can convert `C x H x W` tensors back into a PIL image with the [to_pil_image](https://pytorch.org/vision/main/generated/torchvision.transforms.functional.to_pil_image.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "executionInfo": {
     "elapsed": 394,
     "status": "ok",
     "timestamp": 1714980717832,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "fsqamRxJ2zf4",
    "outputId": "75700cd2-85ad-4866-efc4-f8ac35863237"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f405cf9c430>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = F.to_pil_image(x_10_tensor) #display PIL image from tensor values\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Previously, we defined a `trans` variable to handle the conversion of an image into a tensor. [Transforms](https://pytorch.org/vision/stable/transforms.html) in torchvision are a set of utility functions that enable various modifications and preprocessing steps on datasets, such as resizing, normalization, and data augmentation. These transformations can be applied to prepare the dataset for use in a neural network.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforms function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways we can apply our list of transforms to a dataset. One such way is to set it to a dataset's `transform` variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ue8Qs8RU0nF7"
   },
   "outputs": [],
   "source": [
    "train_set.transform = trans\n",
    "valid_set.transform = trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Think of a dataset as a deck of flashcards. A [DataLoader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#preparing-your-data-for-training-with-dataloaders) determines how cards are drawn from this deck to train a machine learning model. While it’s possible to process the entire dataset at once, this approach is computationally expensive. Instead, using smaller batches of data has been shown to improve training efficiency ([source](https://arxiv.org/pdf/1804.07612)).\n",
    "\n",
    "For instance, if we set a `batch_size` of 32, the model is trained by shuffling the deck and drawing 32 cards at a time. Shuffling is unnecessary for validation since the model is not learning during this phase, but a batch size is still used to manage memory efficiently.\n",
    "\n",
    "The optimal batch size depends on the specific problem and resources available. However, values like 32 or 64 are commonly sufficient for many tasks and are default settings in some machine learning frameworks. For simplicity, we’ll use a batch size of 32 in this example.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "vhfKSYKCpHxf"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "Now, let’s build the model! Neural networks consist of layers, where each layer performs specific mathematical operations on the input data before passing it to the next layer. To begin, we’ll construct a basic model consisting of four key components:\n",
    "\n",
    "1. A [Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) layer that converts multi-dimensional input data into a one-dimensional vector.\n",
    "2. An **input layer**, which serves as the first layer of neurons receiving the input data.\n",
    "3. A **hidden layer**, an intermediate layer of neurons that processes the data and helps the model learn complex patterns.\n",
    "4. An **output layer**, the final layer that produces the model's predictions.\n",
    "\n",
    "We’ll store these components in a `layers` variable, which will hold the list of layers for our model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = []\n",
    "layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we looked at the shape of our data above, we saw the images had 3 dimensions: `C x H x W`. To flatten an image means to combine all of these images into 1 dimension. Let's say we have a tensor like the one below. Try running the code cell to see what it looks like before and after being flattened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_matrix = torch.tensor(\n",
    "    [[1, 2, 3],\n",
    "     [4, 5, 6],\n",
    "     [7, 8, 9]]\n",
    ")\n",
    "test_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Flatten()(test_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "Why didn’t anything happen? Neural networks are designed to process data in batches, but currently, the Flatten layer is seeing three separate vectors instead of a single 2D matrix. To resolve this, we need to \"batch\" the data by introducing an additional dimension. \n",
    "\n",
    "This can be achieved by adding a new pair of brackets to the data. Since `test_matrix` is already a tensor, we can use the shorthand method below. The keyword `None` creates a new dimension, while `:` selects all the elements along that dimension. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3],\n",
       "         [4, 5, 6],\n",
       "         [7, 8, 9]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_test_matrix = test_matrix[None, :]\n",
    "batch_test_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Flatten()(batch_test_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've gotten the hang of the `Flatten` layer, let's add it to our list of `layers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Flatten(start_dim=1, end_dim=-1)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = [\n",
    "    nn.Flatten()\n",
    "]\n",
    "layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Input Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "The first layer of neurons in our model connects the flattened input image to the rest of the network. For this, we’ll use a [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layer, which is a *fully connected layer*. In a fully connected layer, each neuron and its corresponding weights influence every neuron in the subsequent layer.\n",
    "\n",
    "To initialize the weights for this layer, PyTorch needs two key pieces of information:\n",
    "1. **The size of the input**: Since our images are flattened, the input size is determined by multiplying the number of channels, the image height (in pixels), and the image width (in pixels).\n",
    "2. **The number of neurons**: This is a parameter we define, determining how many neurons the layer will contain.\n",
    "\n",
    "This layer will serve as the foundation for processing the input data and passing it to the deeper layers of the network.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1 * 28 * 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "Deciding on the right number of neurons is part of the \"science\" in data science, as it involves balancing the model's capacity to capture the dataset's complexity. For now, we’ll use `512` neurons in this layer. Later, you can experiment with this value to observe its impact on training and begin understanding how this choice influences the model's performance.\n",
    "\n",
    "We’ll also introduce an **activation function**, which helps the network learn more complex patterns. While we’ll dive deeper into activation functions later, for now, we’ll use the [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) (Rectified Linear Unit) activation function. ReLU is widely used because it enables the network to model non-linear relationships in the data, leading to more accurate predictions compared to relying on strictly linear transformations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Flatten(start_dim=1, end_dim=-1),\n",
       " Linear(in_features=784, out_features=512, bias=True),\n",
       " ReLU()]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = [\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(input_size, 512),  # Input images\n",
    "    nn.ReLU(),  # Activation for input\n",
    "]\n",
    "layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "Next, we’ll include another fully connected (dense) linear layer in our model. In a later lesson, we’ll explore why adding additional layers of neurons can enhance a model’s ability to learn and capture patterns in the data.\n",
    "\n",
    "Similar to the input layer, the hidden layer's [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) needs to know the size of the data it will process. Specifically, the number of inputs to the hidden layer matches the number of neurons in the previous layer, as each neuron in the previous layer contributes one value to the next layer. This ensures the layers are properly connected and can pass data effectively through the network.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Flatten(start_dim=1, end_dim=-1),\n",
       " Linear(in_features=784, out_features=512, bias=True),\n",
       " ReLU(),\n",
       " Linear(in_features=512, out_features=512, bias=True),\n",
       " ReLU()]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = [\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(input_size, 512),  # Input\n",
    "    nn.ReLU(),  # Activation for input\n",
    "    nn.Linear(512, 512),  # Hidden\n",
    "    nn.ReLU()  # Activation for hidden\n",
    "]\n",
    "layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Output Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Finally, we’ll add the output layer to the model. Since our task is to classify an input image into one of 10 possible categories, the output layer will have 10 neurons—one for each class. The model’s prediction is based on the neuron with the highest output value. A higher value for a specific neuron indicates the model’s confidence that the input image belongs to the class assigned to that neuron.\n",
    "\n",
    "Unlike the previous layers, we won’t apply the `relu` activation function to the output layer. Instead, a **loss function** will be used to process the outputs, which we’ll discuss in the next section. The loss function is essential for guiding the model’s learning by comparing predictions to the true labels and calculating how far off the predictions are.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Flatten(start_dim=1, end_dim=-1),\n",
       " Linear(in_features=784, out_features=512, bias=True),\n",
       " ReLU(),\n",
       " Linear(in_features=512, out_features=512, bias=True),\n",
       " ReLU(),\n",
       " Linear(in_features=512, out_features=10, bias=True)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = 10\n",
    "\n",
    "layers = [\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(input_size, 512),  # Input\n",
    "    nn.ReLU(),  # Activation for input\n",
    "    nn.Linear(512, 512),  # Hidden\n",
    "    nn.ReLU(),  # Activation for hidden\n",
    "    nn.Linear(512, n_classes)  # Output\n",
    "]\n",
    "layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the Basic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) model expects a sequence of arguments, not a list, so we can use the [* operator](https://docs.python.org/3/reference/expressions.html#expression-lists) to unpack our list of layers into a sequence. We can print the model to verify these layers loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "7lmQ-G_FuThy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): Linear(in_features=784, out_features=512, bias=True)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(*layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGSKooDz1DH5"
   },
   "source": [
    "Much like tensors, when the model is first initialized, it will be processed on a CPU. To have it process with a GPU, we can use `to(device)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 154,
     "status": "ok",
     "timestamp": 1714980725791,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "ynuW56udz7C1",
    "outputId": "25675910-e29a-4f3d-99c8-2254d5277e38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): Linear(in_features=784, out_features=512, bias=True)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check which device a model is on, we can check which device the model parameters are on. Check out this [stack overflow](https://stackoverflow.com/questions/58926054/how-to-get-the-device-type-of-a-pytorch-module-conveniently) post for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1714980916077,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "VlCPs5KDz9Ii",
    "outputId": "67e73ac9-139e-4ad4-9ff6-d344abd267c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Basic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "With the training and validation data prepared, and our model defined, it’s time to train the model using the training data and evaluate its performance with the validation data.\n",
    "\n",
    "The process of \"training a model\" is often referred to as \"fitting the model to the data.\" This term emphasizes that the model's parameters are adjusted during training to better represent and understand the patterns in the data it is provided. Over time, these adjustments enable the model to make increasingly accurate predictions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Similar to how teachers grade students' work, we need a way to evaluate the model's predictions. This evaluation is done using a `loss function`. A loss function measures how far off the model's predictions are from the true answers. \n",
    "\n",
    "For this task, we’ll use [CrossEntropy](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), a loss function specifically designed for classification problems. It calculates how well the model predicts the correct category from a set of possible categories, providing feedback that guides the model's learning process.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "jG8Oxrz0z_y2"
   },
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we select an `optimizer` for our model. If the `loss_function` provides a grade, the optimizer tells the model how to learn from this grade to do better next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "Although the loss function provides valuable feedback for the model to learn, its results can be difficult for humans to interpret. For this reason, data scientists often include additional metrics, such as accuracy, to better understand the model's performance.\n",
    "\n",
    "To calculate accuracy, we compare the number of correct predictions made by the model to the total number of predictions. Since we process data in batches during training and evaluation, accuracy can be calculated for each batch and then aggregated.\n",
    "\n",
    "The total number of predictions corresponds to the size of the dataset. Let’s represent this as `N`. The `batch size`, denoted as `n`, determines how many samples are processed at a time. By keeping track of these values, we can calculate the overall accuracy efficiently.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_N = len(train_loader.dataset)\n",
    "valid_N = len(valid_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "print(train_N) #Total training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(valid_N) #Total testing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Next, we’ll create a function to compute the accuracy for each batch. This function will calculate the fraction of correct predictions within the batch. By summing up these batch-level accuracies, we can determine the overall accuracy across the entire dataset. This approach ensures that accuracy is calculated efficiently, even when working with data in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_accuracy(output, y, N):\n",
    "    pred = output.argmax(dim=1, keepdim=True)\n",
    "    correct = pred.eq(y.view_as(pred)).sum().item()\n",
    "    return correct / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Train Function to train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "8iPLK1V53w3R"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    model.train()\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        output = model(x)\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = loss_function(output, y)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss += batch_loss.item()\n",
    "        accuracy += get_batch_accuracy(output, y, train_N)\n",
    "    print('Train - Loss: {:.4f} Accuracy: {:.4f}'.format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Validate Function for validating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "8WlmJPR5yZJ5"
   },
   "outputs": [],
   "source": [
    "def validate():\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in valid_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output = model(x)\n",
    "\n",
    "            loss += loss_function(output, y).item()\n",
    "            accuracy += get_batch_accuracy(output, y, valid_N)\n",
    "    print('Valid - Loss: {:.4f} Accuracy: {:.4f}'.format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "To monitor the model's progress, we will alternate between training and validation. Just as a student may need to review their deck of flashcards several times to fully grasp the concepts, the model must pass through the training data multiple times to improve its understanding.\n",
    "\n",
    "An **epoch** refers to one complete pass through the entire dataset. Let’s train and validate the model for 10 epochs, allowing us to observe how it learns and refines its predictions over time.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57767,
     "status": "ok",
     "timestamp": 1714814236517,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "SElV0J_aw-bW",
    "outputId": "452e3897-934b-4823-8a75-43a20551f06f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train - Loss: 377.3763 Accuracy: 0.9385\n",
      "Valid - Loss: 32.1754 Accuracy: 0.9679\n",
      "Epoch: 1\n",
      "Train - Loss: 155.6835 Accuracy: 0.9742\n",
      "Valid - Loss: 28.6509 Accuracy: 0.9714\n",
      "Epoch: 2\n",
      "Train - Loss: 110.5302 Accuracy: 0.9815\n",
      "Valid - Loss: 21.6697 Accuracy: 0.9799\n",
      "Epoch: 3\n",
      "Train - Loss: 81.5046 Accuracy: 0.9855\n",
      "Valid - Loss: 33.1957 Accuracy: 0.9714\n",
      "Epoch: 4\n",
      "Train - Loss: 69.6247 Accuracy: 0.9880\n",
      "Valid - Loss: 23.4122 Accuracy: 0.9787\n",
      "Epoch: 5\n",
      "Train - Loss: 52.8135 Accuracy: 0.9909\n",
      "Valid - Loss: 23.4329 Accuracy: 0.9795\n",
      "Epoch: 6\n",
      "Train - Loss: 48.4653 Accuracy: 0.9920\n",
      "Valid - Loss: 29.2641 Accuracy: 0.9777\n",
      "Epoch: 7\n",
      "Train - Loss: 41.7732 Accuracy: 0.9931\n",
      "Valid - Loss: 28.9173 Accuracy: 0.9787\n",
      "Epoch: 8\n",
      "Train - Loss: 39.9343 Accuracy: 0.9934\n",
      "Valid - Loss: 25.7802 Accuracy: 0.9807\n",
      "Epoch: 9\n",
      "Train - Loss: 30.6754 Accuracy: 0.9950\n",
      "Valid - Loss: 27.6463 Accuracy: 0.9829\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "torch.set_float32_matmul_precision('high')\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    train()\n",
    "    validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing it on our original sample. We can use our model like a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1714815283682,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "FPKjh3TN1_Sx",
    "outputId": "729b2cfa-edcf-44f4-ebcc-c63538081e5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-43.0433, -16.9647, -14.6090,  24.3362, -21.2535,  -7.2119, -46.1297,\n",
       "         -18.0070, -21.0873, -10.0882]], device='cuda:0',\n",
       "       grad_fn=<CompiledFunctionBackward>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model(x_10_gpu)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should be ten numbers, each corresponding to a different output neuron. Thanks to how the data is structured, the index of each number matches the corresponding handwritten number. The 0th index is a prediction for a handwritten 0, the 1st index is a prediction for a handwritten 1, and so on.\n",
    "\n",
    "We can use the `argmax` function to find the index of the highest value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1101,
     "status": "ok",
     "timestamp": 1714815292555,
     "user": {
      "displayName": "Danielle Detering US",
      "userId": "15432464718872067879"
     },
     "user_tz": 420
    },
    "id": "XrmW1TrN2OOr",
    "outputId": "d0d8dc74-e88c-47db-f249-6e83d4f0dc44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3]], device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.argmax(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did it get it right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8O7pADY2zgL"
   },
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yu5NNukp2zgL"
   },
   "source": [
    "MNIST is not only useful for its historical influence on Computer Vision, but it's also a great [benchmark](http://www.cs.toronto.edu/~serailhydra/publications/tbd-iiswc18.pdf) and debugging tool. Having trouble getting a fancy new machine learning architecture working? Check it against MNIST. If it can't learn on this dataset, chances are it won't learn on more complicated images and datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lV4QiOtg2zgL"
   },
   "source": [
    "### Clear the Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3iQrnws2zgL"
   },
   "source": [
    "Before moving on, please execute the following cell to clear up the GPU memory. This is required to move on to the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "JCvpuSx-2zgM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "“introduction”",
   "language": "python",
   "name": "jupyter-notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
